{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of the Q-Learning Algorithm for Atari Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started on device: cpu\n",
      "0.4.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Started on device:\", device)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this method calculates in which episodes a video should be recorded\n",
    "def video_callable(episode_id):\n",
    "    return episode_id % 20 == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the image processor to crop and resize an image and convert it to grayscale\n",
    "class ImageProcessor:\n",
    "\n",
    "    def __init__(self, size, device):\n",
    "        self.transformations = T.Compose([T.ToPILImage(),\n",
    "                                          T.Grayscale(),\n",
    "                                          T.Resize(size),\n",
    "                                          T.ToTensor()])\n",
    "        self.device = device\n",
    "\n",
    "    def process(self, screen):\n",
    "        # crop the image to a square image\n",
    "        screen = screen[34:-16, :, :]\n",
    "        return self.transformations(screen).squeeze(0).to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a single transition from one state into the next state with a given action\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the experience replay memory to save transitions and sample a random batch from them\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the DQN architecture with the forward function\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc4 = nn.Linear(3136, 512)\n",
    "        self.fc5 = nn.Linear(512, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.fc4(x.view(x.size(0), -1)))\n",
    "        return self.fc5(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dueling DQN architecture with the forward function\n",
    "# https://arxiv.org/abs/1511.06581\n",
    "class DuelingDQN(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, num_actions):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "\n",
    "        self.fc1_adv = nn.Linear(3136, 512)\n",
    "        self.fc1_val = nn.Linear(3136, 512)\n",
    "\n",
    "        self.fc2_adv = nn.Linear(512, num_actions)\n",
    "        self.fc2_val = nn.Linear(512, 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        adv = self.relu(self.fc1_adv(x))\n",
    "        val = self.relu(self.fc1_val(x))\n",
    "\n",
    "        adv = self.fc2_adv(adv)\n",
    "        val = self.fc2_val(val).expand(x.size(0), self.num_actions)\n",
    "\n",
    "        x = val + adv - adv.mean(1).unsqueeze(1).expand(x.size(0), self.num_actions)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Gym Environment:\n",
    "\n",
    "https://gym.openai.com/docs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ..., \n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       [[228, 111, 111],\n",
       "        [228, 111, 111],\n",
       "        [228, 111, 111],\n",
       "        ..., \n",
       "        [228, 111, 111],\n",
       "        [228, 111, 111],\n",
       "        [228, 111, 111]],\n",
       "\n",
       "       [[228, 111, 111],\n",
       "        [228, 111, 111],\n",
       "        [228, 111, 111],\n",
       "        ..., \n",
       "        [228, 111, 111],\n",
       "        [228, 111, 111],\n",
       "        [228, 111, 111]],\n",
       "\n",
       "       ..., \n",
       "       [[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ..., \n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ..., \n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ..., \n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]]], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Atari Actions: 0 (noop), 1 (fire), 2 (left) and 3 (right) are valid actions for Breakout\n",
    "env_name = \"MsPacman-v0\"\n",
    "env = gym.make(env_name)\n",
    "\n",
    "#start video recording\n",
    "video_path = os.getcwd() + \"/recording-\" + env_name\n",
    "env = gym.wrappers.Monitor(env, video_path, force=True)\n",
    "\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the hyperparameters for the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "# the discount factor of the MDP\n",
    "GAMMA = 0.99\n",
    "\n",
    "# parameters for the exploration strategy\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.02\n",
    "EPS_DECAY = 100000\n",
    "\n",
    "# update the DQN network every 1000 steps\n",
    "TARGET_UPDATE = 1000\n",
    "\n",
    "# the input channels of the DQN\n",
    "IN_CHANNELS = 4\n",
    "\n",
    "# sample at the beginning 10000 Transitions with random actions for the replay memory\n",
    "LEARNING_STARTS = 10000\n",
    "\n",
    "# number of training episodes (= full games)\n",
    "num_episodes = 10000\n",
    "\n",
    "# just for observation\n",
    "steps_done = 0\n",
    "steps_trained = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the DQN, image processor, optimizer and replay memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 84 is the used size in DQN paper\n",
    "image_processor = ImageProcessor(84, device)\n",
    "\n",
    "# create the two networks for the iterative update\n",
    "policy_net = DuelingDQN(IN_CHANNELS, env.action_space.n).float().to(device)\n",
    "target_net = DuelingDQN(IN_CHANNELS, env.action_space.n).float().to(device)\n",
    "\n",
    "# load the state of the polciy net into the target net, the target net gets updated each  TARGET_UPDATE steps\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=0.0001)\n",
    "memory = ReplayMemory(100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select an action:\n",
    "\n",
    "This method selects with an probality a random action or the DQN will choose the next action. `eps_threshold` should get smaller over time and at the end only the DQN should choose the next actions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(current_state):\n",
    "    global steps_done\n",
    "    global steps_trained\n",
    "    sample = random.random()\n",
    "    \n",
    "    # calculate the epsilon for the exploration\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_trained / EPS_DECAY)\n",
    "    if LEARNING_STARTS < steps_done:\n",
    "        steps_trained += 1\n",
    "\n",
    "    steps_done += 1\n",
    "    if LEARNING_STARTS < steps_done and sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(current_state).max(1)[1].view(-1, 1)\n",
    "    else:\n",
    "        return torch.LongTensor([[random.randrange(env.action_space.n)]]).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize the model:\n",
    "\n",
    "This method will optimize the model. It samples from the memory the batch size which will be used for training. The zip function is used to transpose the batch in the correct format (https://www.programiz.com/python-programming/methods/built-in/zip). Double DQN is used as a standard because the training is faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(double_dqn=True):\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "\n",
    "    # Transpose the batch\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    state_batch = Variable(torch.from_numpy(np.asarray(batch.state)).to(device))\n",
    "    action_batch = Variable(torch.from_numpy(np.asarray(batch.action)).long().to(device))\n",
    "    reward_batch = Variable(torch.from_numpy(np.asarray(batch.reward)).to(device))\n",
    "    done_batch = Variable(torch.from_numpy(np.asarray(batch.done)).float().to(device))\n",
    "    next_state_batch = Variable(torch.from_numpy(np.asarray(batch.next_state)).to(device))\n",
    "\n",
    "    # get the right q values for the selected actions\n",
    "    q_s_a = policy_net(state_batch).gather(1, action_batch.unsqueeze(1))\n",
    "    q_s_a = q_s_a.squeeze()\n",
    "\n",
    "    if double_dqn:\n",
    "        # get the Q values for best actions in next_state_batch\n",
    "        # based off the current Q network\n",
    "        # max(Q(s', a', theta_i)) wrt a'\n",
    "        q_next_state_values = policy_net(next_state_batch).detach()\n",
    "        _, a_prime = q_next_state_values.max(1)\n",
    "        # get Q values from frozen network for next state and chosen action\n",
    "        # Q(s',argmax(Q(s',a', theta_i), theta_i_frozen)) (argmax wrt a')\n",
    "        q_target_next_state_values = target_net(next_state_batch).detach()\n",
    "        q_target_s_a_prime = q_target_next_state_values.gather(1, a_prime.unsqueeze(1))\n",
    "        q_target_s_a_prime = q_target_s_a_prime.squeeze()\n",
    "\n",
    "        # if current state is end of episode, then there is no next Q value\n",
    "        q_target_s_a_prime = (1 - done_batch) * q_target_s_a_prime\n",
    "\n",
    "        error = (reward_batch + (GAMMA * q_target_s_a_prime)) - q_s_a\n",
    "    else:\n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the columns of actions taken\n",
    "        next_state_values = target_net(next_state_batch).detach()\n",
    "        q_s_a_prime, a_prime = next_state_values.max(1)\n",
    "        q_s_a_prime = (1 - done_batch) * q_s_a_prime\n",
    "\n",
    "        # Compute Bellman error\n",
    "        # r + gamma * Q(s',a', theta_i_frozen) - Q(s, a, theta_i)\n",
    "        error = (reward_batch + (GAMMA * q_s_a_prime)) - q_s_a\n",
    "\n",
    "    # clip the error and flip\n",
    "    clipped_error = -1.0 * error.clamp(-1, 1)\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    q_s_a.backward(clipped_error.data)\n",
    "\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the training of the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  0\n",
      "Reward: 0.0 Episode: 0 Steps done: 1\n",
      "Reward: 0.0 Episode: 0 Steps done: 2\n",
      "Reward: 0.0 Episode: 0 Steps done: 3\n",
      "Reward: 0.0 Episode: 0 Steps done: 4\n",
      "Reward: 0.0 Episode: 0 Steps done: 5\n",
      "Reward: 0.0 Episode: 0 Steps done: 6\n",
      "Reward: 0.0 Episode: 0 Steps done: 7\n",
      "Reward: 0.0 Episode: 0 Steps done: 8\n",
      "Reward: 0.0 Episode: 0 Steps done: 9\n",
      "Reward: 0.0 Episode: 0 Steps done: 10\n",
      "Reward: 0.0 Episode: 0 Steps done: 11\n",
      "Reward: 0.0 Episode: 0 Steps done: 12\n",
      "Reward: 0.0 Episode: 0 Steps done: 13\n",
      "Reward: 0.0 Episode: 0 Steps done: 14\n",
      "Reward: 0.0 Episode: 0 Steps done: 15\n",
      "Reward: 0.0 Episode: 0 Steps done: 16\n",
      "Reward: 0.0 Episode: 0 Steps done: 17\n",
      "Reward: 0.0 Episode: 0 Steps done: 18\n",
      "Reward: 0.0 Episode: 0 Steps done: 19\n",
      "Reward: 0.0 Episode: 0 Steps done: 20\n",
      "Reward: 0.0 Episode: 0 Steps done: 21\n",
      "Reward: 0.0 Episode: 0 Steps done: 22\n",
      "Reward: 0.0 Episode: 0 Steps done: 23\n",
      "Reward: 0.0 Episode: 0 Steps done: 24\n",
      "Reward: 0.0 Episode: 0 Steps done: 25\n",
      "Reward: 0.0 Episode: 0 Steps done: 26\n",
      "Reward: 0.0 Episode: 0 Steps done: 27\n",
      "Reward: 0.0 Episode: 0 Steps done: 28\n",
      "Reward: 0.0 Episode: 0 Steps done: 29\n",
      "Reward: 0.0 Episode: 0 Steps done: 30\n",
      "Reward: 0.0 Episode: 0 Steps done: 31\n",
      "Reward: 0.0 Episode: 0 Steps done: 32\n",
      "Reward: 0.0 Episode: 0 Steps done: 33\n",
      "Reward: 0.0 Episode: 0 Steps done: 34\n",
      "Reward: 0.0 Episode: 0 Steps done: 35\n",
      "Reward: 0.0 Episode: 0 Steps done: 36\n",
      "Reward: 0.0 Episode: 0 Steps done: 37\n",
      "Reward: 0.0 Episode: 0 Steps done: 38\n",
      "Reward: 0.0 Episode: 0 Steps done: 39\n",
      "Reward: 0.0 Episode: 0 Steps done: 40\n",
      "Reward: 0.0 Episode: 0 Steps done: 41\n",
      "Reward: 0.0 Episode: 0 Steps done: 42\n",
      "Reward: 0.0 Episode: 0 Steps done: 43\n",
      "Reward: 0.0 Episode: 0 Steps done: 44\n",
      "Reward: 0.0 Episode: 0 Steps done: 45\n",
      "Reward: 0.0 Episode: 0 Steps done: 46\n",
      "Reward: 0.0 Episode: 0 Steps done: 47\n",
      "Reward: 0.0 Episode: 0 Steps done: 48\n",
      "Reward: 0.0 Episode: 0 Steps done: 49\n",
      "Reward: 0.0 Episode: 0 Steps done: 50\n",
      "Reward: 0.0 Episode: 0 Steps done: 51\n",
      "Reward: 0.0 Episode: 0 Steps done: 52\n",
      "Reward: 0.0 Episode: 0 Steps done: 53\n",
      "Reward: 0.0 Episode: 0 Steps done: 54\n",
      "Reward: 0.0 Episode: 0 Steps done: 55\n",
      "Reward: 0.0 Episode: 0 Steps done: 56\n",
      "Reward: 0.0 Episode: 0 Steps done: 57\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d468f6a9d1b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# Perform one step of the optimization (on the target network)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0moptimize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# Update the target network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-b3aad0221858>\u001b[0m in \u001b[0;36moptimize_model\u001b[0;34m(double_dqn)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mq_s_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipped_error\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/condatascience/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/condatascience/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i_episode in range(num_episodes):\n",
    "    print(\"Episode: \", i_episode)\n",
    "    # Initialize the environment and state\n",
    "    state = env.reset()\n",
    "    state = image_processor.process(state)\n",
    "    \n",
    "    # stack the first image 4 times as it is the only one\n",
    "    state = np.stack([state]*4, axis=0)\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(torch.FloatTensor(state).unsqueeze(0).to(device))\n",
    "        next_state, reward, done, _ = env.step(action.item())\n",
    "        reward = torch.Tensor([reward], device=device)\n",
    "        print(\"Reward:\", reward.item(), \"Episode:\", i_episode, \"Steps done:\", steps_done)\n",
    "        # Observe new state\n",
    "        next_state = image_processor.process(next_state)\n",
    "        \n",
    "        # append the new pixels from the environment\n",
    "        next_state = np.append(state[1:, :, :], np.expand_dims(next_state, 0), axis=0)\n",
    "\n",
    "        if done is False:\n",
    "            done = 0\n",
    "        else:\n",
    "            done = 1\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward, done)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Update the target network\n",
    "        if steps_done % TARGET_UPDATE == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        if done:\n",
    "            print(\"Done\")\n",
    "            break\n",
    "\n",
    "    torch.save(target_net.state_dict(), os.getcwd() + '/' + env_name + '_dueling_ddqn')\n",
    "\n",
    "print('Complete')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
